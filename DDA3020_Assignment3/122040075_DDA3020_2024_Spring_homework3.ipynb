{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbCV8M0vukk1"
   },
   "source": [
    "# DDA 3020 Assignment 3: Fully-Connected Neural Networks and CNN\n",
    "\n",
    "Before we start, please put your name and ID in following format:\n",
    "\n",
    "Firstname LASTNAME, #00000000   //  e.g.) Justin JOHNSON, #12345678\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAMIlt0zxpNc"
   },
   "source": [
    "# Your Answer\n",
    "\n",
    "Boshi Xu, #122040075"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjgisxL9t5W0"
   },
   "source": [
    "# Overview\n",
    "\n",
    "In this exercise, we are going to build a **fully-connected neural network** and a **convolutional neural network** from scratch using **numpy** package.\n",
    "\n",
    "For **Fully-connected Neural Network**, you need to implement both forward and backward part to train your model and update the parameters.\n",
    "\n",
    "For **Convolutional Neural Network**, you only need to implement the forward part, including `convolutional layers`, `max_pooling` and `fully-connected layers`. Functions of implementing backpropagation are given. Please check the input requirements for those functions, especially pay attention to the dimension.\n",
    "\n",
    "Please follow the guidance and finish the **<font color=\"green\">[To Do]</font>** and **<font color=\"red\">[Task]</font>** part. Please feel free to define your own functions or modify the functions given if needed. No writing report is required and you will get the mark if the whole process and results are reasonable.\n",
    "\n",
    "If you meet any package or memory issues that can't be solved, feel free to use this colab script to finish your task (remember to copy your own one instead of modifying this colab script directly) : https://colab.research.google.com/drive/1AdGyiVYosU2uMBQ9ahx5B7ZOmH8zEQg_?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZvuOyO6xv0B"
   },
   "source": [
    "# Set Up Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3W80z-YfOVyw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "C:\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5si-RE7LQOTV"
   },
   "source": [
    "# Fully-connected Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgdmbFCBI0RL"
   },
   "source": [
    "# 1.1 Loading Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wasX27MVGMzE"
   },
   "source": [
    "In this assignment, we mainly adopt the FashionMnist dataset to train our network. You will be given two csv files: **fashion-mnist_train.csv; fashion-mnist_test.csv**, containing 60000 and 10000 samples each. Each sample is a 28 * 28 grayscale image. In the csv file, each row represents a sample with dimension 1 * 784 (28*28). The first step of this assignment is loading our data. Please run the below cells and a visualization code is also given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17045,
     "status": "ok",
     "timestamp": 1711826794887,
     "user": {
      "displayName": "物午",
      "userId": "12077501645191558009"
     },
     "user_tz": -480
    },
    "id": "W1ZeWHG1ycc1",
    "outputId": "73ad1445-d94a-49da-8d17-9455d513ba44"
   },
   "outputs": [],
   "source": [
    "# run it only when you use colab to do this assignment\n",
    "# no need to run it if you do this assignment locally\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qw023ZBHQMQx"
   },
   "outputs": [],
   "source": [
    "mnist_train = pd.read_csv('fashion-mnist_train.csv') # change the path to your own path\n",
    "mnist_test = pd.read_csv('fashion-mnist_test.csv') # change the path to your own path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 492,
     "status": "ok",
     "timestamp": 1711826840596,
     "user": {
      "displayName": "物午",
      "userId": "12077501645191558009"
     },
     "user_tz": -480
    },
    "id": "nH6XUKXXZDjd",
    "outputId": "75a03b9b-6830-49ae-f9b0-14bcb55661cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n",
      "(60000, 785)\n"
     ]
    }
   ],
   "source": [
    "# check the shape of of the training data 检查形状\n",
    "print(mnist_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1711826845179,
     "user": {
      "displayName": "物午",
      "userId": "12077501645191558009"
     },
     "user_tz": -480
    },
    "id": "RKUB3KTuZA_i",
    "outputId": "41fd5417-6467-4abc-86b4-f17529125788"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027451</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.027451</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.027451</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.650980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel1  pixel2  pixel3    pixel4    pixel5    pixel6    pixel7    pixel8  \\\n",
       "0     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.019608   \n",
       "3     0.0     0.0     0.0  0.003922  0.007843  0.000000  0.000000  0.000000   \n",
       "4     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5     0.0     0.0     0.0  0.019608  0.015686  0.019608  0.019608  0.011765   \n",
       "6     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.011765  0.007843   \n",
       "9     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     pixel9   pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.117647  0.168627   \n",
       "3  0.000000  0.000000  ...  0.011765  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.019608  0.023529  ...  0.027451  0.031373  0.027451  0.015686  0.011765   \n",
       "6  0.000000  0.000000  ...  0.054902  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.000000  ...  0.003922  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.000000  ...  0.796078  0.839216  0.650980  0.000000  0.000000   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0  0.000000  0.000000       0.0       0.0       0.0  \n",
       "1  0.000000  0.000000       0.0       0.0       0.0  \n",
       "2  0.000000  0.000000       0.0       0.0       0.0  \n",
       "3  0.003922  0.000000       0.0       0.0       0.0  \n",
       "4  0.000000  0.000000       0.0       0.0       0.0  \n",
       "5  0.027451  0.019608       0.0       0.0       0.0  \n",
       "6  0.000000  0.000000       0.0       0.0       0.0  \n",
       "7  0.000000  0.000000       0.0       0.0       0.0  \n",
       "8  0.000000  0.000000       0.0       0.0       0.0  \n",
       "9  0.000000  0.000000       0.0       0.0       0.0  \n",
       "\n",
       "[10 rows x 784 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.168627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027451</td>\n",
       "      <td>0.031373</td>\n",
       "      <td>0.027451</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.027451</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.650980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel1  pixel2  pixel3    pixel4    pixel5    pixel6    pixel7    pixel8  \\\n",
       "0     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.019608   \n",
       "3     0.0     0.0     0.0  0.003922  0.007843  0.000000  0.000000  0.000000   \n",
       "4     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5     0.0     0.0     0.0  0.019608  0.015686  0.019608  0.019608  0.011765   \n",
       "6     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "7     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.011765  0.007843   \n",
       "9     0.0     0.0     0.0  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     pixel9   pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.117647  0.168627   \n",
       "3  0.000000  0.000000  ...  0.011765  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.019608  0.023529  ...  0.027451  0.031373  0.027451  0.015686  0.011765   \n",
       "6  0.000000  0.000000  ...  0.054902  0.000000  0.000000  0.000000  0.000000   \n",
       "7  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "8  0.000000  0.000000  ...  0.003922  0.000000  0.000000  0.000000  0.000000   \n",
       "9  0.000000  0.000000  ...  0.796078  0.839216  0.650980  0.000000  0.000000   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0  0.000000  0.000000       0.0       0.0       0.0  \n",
       "1  0.000000  0.000000       0.0       0.0       0.0  \n",
       "2  0.000000  0.000000       0.0       0.0       0.0  \n",
       "3  0.003922  0.000000       0.0       0.0       0.0  \n",
       "4  0.000000  0.000000       0.0       0.0       0.0  \n",
       "5  0.027451  0.019608       0.0       0.0       0.0  \n",
       "6  0.000000  0.000000       0.0       0.0       0.0  \n",
       "7  0.000000  0.000000       0.0       0.0       0.0  \n",
       "8  0.000000  0.000000       0.0       0.0       0.0  \n",
       "9  0.000000  0.000000       0.0       0.0       0.0  \n",
       "\n",
       "[10 rows x 784 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the dataset into training set and testing set\n",
    "# for each pixel, the purpose of dividing it by 255 is to scale its value between 0 and 1 since the maximun value is 255\n",
    "# for testset, since its label is a number, we first transform it into one-hot vector\n",
    "# 将数据集转换为训练集和测试集\n",
    "# 对于每个像素，将其除以 255 的目的是将其值缩放在 0 和 1 之间，因为最大值为 255\n",
    "# 对于测试集，由于它的标签是数字，我们首先将其转换为one-hot向量\n",
    "\n",
    "X_train, Y_train = mnist_train.drop('label',axis=1)/255, mnist_train['label']\n",
    "X_test, Y_test = mnist_test.drop('label',axis=1)/255, mnist_test['label']\n",
    "Y_train, Y_test = pd.get_dummies(Y_train),pd.get_dummies(Y_test)\n",
    "\n",
    "# you can check the first 10 samples of the training set\n",
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "z6bv_YnBKKga"
   },
   "outputs": [],
   "source": [
    "# here is a function of reshaping the 1*784 vector into 28*28 matrix (the original format of a grayscale image)\n",
    "# 将1*784向量 变成28*28矩阵（灰度图像的原始格式）的函数\n",
    "def reshape_data(X_train, X_test):\n",
    "  train_size, test_size = X_train.shape[0], X_test.shape[0]\n",
    "  reshape_train, reshape_test = np.transpose(X_train,(1,0)), np.transpose(X_test,(1,0))\n",
    "  reshape_train, reshape_test = reshape_train.reshape((28,28,1,train_size)), reshape_test.reshape((28,28,1,test_size))\n",
    "  return np.transpose(reshape_train,(3,0,1,2)), np.transpose(reshape_test,(3,0,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1711826890245,
     "user": {
      "displayName": "物午",
      "userId": "12077501645191558009"
     },
     "user_tz": -480
    },
    "id": "3rj45iFQJ1u6",
    "outputId": "a253325d-2b78-4583-ddda-9dc00376a9a7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here is a visualization cell that you can check the data sample from FashionMnist\n",
    "\n",
    "x_train_sample, x_test_sample = reshape_data(X_train.values, X_test.values)\n",
    "y_train_sample, y_test_sample = Y_train.values, Y_test.values\n",
    "sample_image = x_train_sample[0,:, :, :].squeeze()\n",
    "\n",
    "# plot the figure\n",
    "plt.imshow(sample_image, cmap='gray')\n",
    "plt.title(\"Visualizing a sample image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L50IwV8QLQGB"
   },
   "source": [
    "# 1.2 Implement Fully-connected Neural Network\n",
    "\n",
    "In this exercise, we will implement a simple **three layer fully-connected neural network** from scratch using only **numpy** package. The NN contains one input layer with 784 nodes (dimension of the input data), one hidden layer with 128 nodes, and one output layer with 10 nodes (10 types of labels). For the first two layers, we will use `relu` as our activation function. The output layer will use `softmax` activation function. The loss for this multi-label prediction task is chosen as softmax-cross-entropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cwxb7EXS85fs"
   },
   "source": [
    "Firstly, we initialize all the parameters using the given functions. We provide an example of using this function. You can modify the function if needed (like changing the dimension etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "I5NFHWJaQuF2"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    '''\n",
    "    initialize all the parameters, including weights and bias\n",
    "\n",
    "    Inputs:\n",
    "    - layer_dims: a list containing the number of nodes in each layer\n",
    "\n",
    "    Example:\n",
    "    - if you want to build a two layer neural network\n",
    "    - the number of nodes in each layer is 20, 10\n",
    "    - then use the function like: para = initialize_parameters([20,10])\n",
    "    - para['W1'] is the weight matrix from layer 1 to layer 2 with dimension (10 * 20)\n",
    "    - you can modify the output dimension if you need\n",
    "\n",
    "    初始化所有参数，包括权重和偏差\n",
    "\n",
    "    输入：\n",
    "    - layer_dims：包含每层节点数的列表\n",
    "\n",
    "    例子：\n",
    "    - 如果你想构建一个两层神经网络\n",
    "    - 每层节点数为20、10\n",
    "    - 然后使用如下函数： para = initialize_parameters([20,10])\n",
    "    - para['W1'] 是从第 1 层到第 2 层的权重矩阵，维度为 (10 * 20)\n",
    "    - 如果需要，您可以修改输出尺寸\n",
    "\n",
    "    '''\n",
    "\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LiwOro3x1HUh"
   },
   "source": [
    "# 1.2.1 Activation functions and Loss functions\n",
    "\n",
    "In this section, you will implement different functions, including `relu`, `softmax` and `cross entropy loss`. The formulas are listed below for reference.\n",
    "\n",
    "---\n",
    "*   $\\text{ReLU}(z) = \\max(0, z)$ \\\\\n",
    "*   $\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$\n",
    "*   $L = -\\sum_{c=1}^{K} y_c \\log(\\hat{y}_c)$, where $y_c$ is the ground truth labels, $\\hat{y}_c$ is the prediced labels\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**<font color=\"red\">[Task]</font>**: Fill the **<font color=\"green\">[TODO]</font>** part in the below three functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YsKrO2HXQxEu"
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "  '''\n",
    "  the ReLU activation function\n",
    "\n",
    "  Inputs:\n",
    "  - Z: the computed results before activation (e.g. WX+b)\n",
    "\n",
    "  Outputs:\n",
    "  - A: the result after activation\n",
    "\n",
    "  '''\n",
    "\n",
    "  A = None\n",
    "\n",
    "  ######################################################################\n",
    "  # TODO: Implement the ReLU Functions              #\n",
    "  ######################################################################\n",
    "  # Replace pass with your code\n",
    "  A = np.maximum(0, Z)\n",
    "  ######################################################################\n",
    "  #         END OF YOUR CODE               #\n",
    "  ######################################################################\n",
    "\n",
    "  return A\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "\n",
    "  '''\n",
    "  the softmax activation function\n",
    "\n",
    "  Inputs:\n",
    "  - Z: the computed results before activation (e.g. WX+b)\n",
    "\n",
    "  Outputs:\n",
    "  - A: the result after activation\n",
    "  softmax函数又称归一化指数函数，是基于 sigmoid 二分类函数在多分类任务上的推广\n",
    "  在多分类网络中，常用 Softmax 作为最后一层进行分类。\n",
    "\n",
    "  '''\n",
    "\n",
    "  #A = None\n",
    "\n",
    "  ######################################################################\n",
    "  # TODO: Implement the softmax Function             #\n",
    "  ######################################################################\n",
    "  # Replace pass with your code\n",
    "  exps = np.exp(Z) \n",
    "  ######################################################################\n",
    "  #         END OF YOUR CODE               #\n",
    "  ######################################################################\n",
    "\n",
    "  return(exps / np.sum(exps, 0))\n",
    "\n",
    "\n",
    "def compute_loss(A, Y):\n",
    "\n",
    "  '''\n",
    "  the softmax activation function\n",
    "\n",
    "  Inputs:\n",
    "  - A: the output result after softmax, the dimension is [10, batch_size]\n",
    "  - Each row of A is a probability vector, A[0,0] represents the probability of the first sample belonging to label 0\n",
    "  - Y: the groundtruth label, the dimension is [10, batch_size]\n",
    "\n",
    "  Outputs:\n",
    "  - L: the computed loss\n",
    "  这里的 L 表示损失函数的值，Y 通常是一个真实的标签向量（在多类分类问题中常为 one-hot 编码形式），\n",
    "  而 A 是模型输出的预测概率（softmax 函数的输出）\n",
    "  损失函数衡量的是预测概率分布与真实概率分布之间的差异。\n",
    "  在机器学习中，优化算法（如梯度下降）会使用这个损失函数的值来更新模型的权重，\n",
    "  目标是最小化这个损失，从而使预测概率尽可能接近真实标签的分布\n",
    "\n",
    "  '''\n",
    "  L = None\n",
    "\n",
    "  ######################################################################\n",
    "  # TODO: Implement the Loss Function               #\n",
    "  ######################################################################\n",
    "  # Replace pass with your code\n",
    "  n = Y.shape[1] #这里 n 是 Y 的第二维度的大小，即样本数（假设每一列代表一个样本）。这一步获取样本数量是为了后续将总损失除以样本数，从而计算平均损失\n",
    "  epsilon = 1e-10 #数值稳定性，防止出现log(0)\n",
    "  L = -np.sum(Y * np.log(A + epsilon))/n \n",
    "  ######################################################################\n",
    "  #         END OF YOUR CODE               #\n",
    "  ######################################################################\n",
    "\n",
    "  return L\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fisu3rKC5n7"
   },
   "source": [
    "# 1.2.2 Forward Propagation\n",
    "\n",
    "In this section, you need to implement the `forward_propagation` function. Each time you propagate from one layer to the next layer, two steps are needed:\n",
    "\n",
    "\n",
    "1.   Multiply the values in the current nodes with the weight matrix and add the bias terms\n",
    "2.   Use the activation function to activate the results\n",
    "\n",
    "In our setting, we have three layers, so we need to propogate our training data twice to get the final prediction results.\n",
    "\n",
    "**<font color=\"red\">[Task]</font>**: Fill the **<font color=\"green\">[TODO]</font>** part in the below function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AKjJoEbDGeaJ"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "\n",
    "    '''\n",
    "    将当前节点中的值与权重矩阵相乘并添加偏差项\n",
    "    使用激活函数激活结果\n",
    "    Inputs:\n",
    "    - X: the training data, with dimension [784,batch_size]\n",
    "    - parameters: a dictionary that contains all the parameters define\n",
    "\n",
    "    Outputs:\n",
    "    - A2: the result of the last layer\n",
    "    - cache: a tuple that stores the propagation results for later backpropagation process\n",
    "\n",
    "    '''\n",
    "    W1, b1, W2, b2 = parameters['W1'], parameters['b1'], parameters['W2'], parameters['b2']\n",
    "\n",
    "\n",
    "    Z1 = None\n",
    "    A1 = None\n",
    "    Z2 = None\n",
    "    A2 = None\n",
    "\n",
    "    ######################################################################\n",
    "    # TODO: Implement the forward propagation            #\n",
    "    ######################################################################\n",
    "    # Replace pass with your code\n",
    "\n",
    "    # First Layer\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.maximum(0, Z1)\n",
    "    \n",
    "    # Second Layer\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = np.maximum(0, Z2)\n",
    "    ######################################################################\n",
    "    #         END OF YOUR CODE               #\n",
    "    ######################################################################\n",
    "\n",
    "\n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2)\n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFp78qaqGlRJ"
   },
   "source": [
    "# 1.2.3 Backpropagation\n",
    "\n",
    "In this section, we will implement the backpropagation process by completing the `relu_derivative` and `backward_propagation` functions.\n",
    "\n",
    "When doing the backpropagation, we mainly use chain rule to compute the gradient. Below is an example that helps you understand this process:\n",
    "\n",
    "Suppose we have two layers, we have the input data $X$, weight matrix $W$, forward result (without activation) $Z$, activation function $σ$, forward result (after activation) $A$, ground truth label $Y$ and Loss $L(Y,A)$. And we have the below relationships:\n",
    "\n",
    "$Z = WX$\n",
    "\n",
    "$A = σ(Z)$\n",
    "\n",
    "If you wanna compute the gradient of the weight matrix, the formula is:\n",
    "\n",
    "$\\frac{dL}{dW} = \\frac{dL}{dA}\\frac{dA}{dZ}\\frac{dZ}{dW} = \\frac{dL}{dA}\\frac{dA}{dZ} × X$, and $\\frac{dL}{dA}$, $\\frac{dA}{dZ}$ can be directly computed according to your loss function and activation function.\n",
    "\n",
    "When implement this function, recall that we have stored the forward result and parameters in the cache. Please fully use these infomation to do your backpropagation.\n",
    "\n",
    "**<font color=\"red\">[Task]</font>**: Fill the **<font color=\"green\">[TODO]</font>** part in the below function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gzTlHq1T0N_c"
   },
   "outputs": [],
   "source": [
    "def relu_derivative(Z):\n",
    "    '''\n",
    "    Inputs:\n",
    "    - Z: the value before activation\n",
    "\n",
    "    Outputs：\n",
    "    - dZ： the derivative of relu function\n",
    "    '''\n",
    "    dZ = np.array(Z, copy=True)\n",
    "\n",
    "    ######################################################################\n",
    "    # TODO: Implement the relu vackward              #\n",
    "    ######################################################################\n",
    "    # Replace pass with your code\n",
    "    pass\n",
    "    ######################################################################\n",
    "    #         END OF YOUR CODE               #\n",
    "    ######################################################################\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    '''\n",
    "    Implement the backward propagation process for our 3 layer NN\n",
    "    Here are several steps you need to do:\n",
    "    1. dZ2: Compute the gradient of Cross Entropy Loss and Softmax Function\n",
    "    2. dW2: Compute the gradient of the weight matrix from hidden layer to output layer\n",
    "    3. db2: Compute the gradient of the bias term from hidden layer to output layer\n",
    "    4. dA2: Compute the gradient of the ReLU activation\n",
    "    ...\n",
    "\n",
    "    Inputs:\n",
    "    - paramters: a dictionary containing all the parameters\n",
    "    - cache: a tuple that store all the forward propagation results\n",
    "    - X: training data, dimension of [784, batch_size]\n",
    "    - Y: ground truth label, dimension of [10, batch_size]\n",
    "\n",
    "    Outputs:\n",
    "    - grads: a dictionary containing all the computed gradients\n",
    "    '''\n",
    "    m = X.shape[1] # batch_size\n",
    "    Z1, A1, W1, b1, Z2, A2, W2, b2 = cache # load the forward results\n",
    "\n",
    "    dZ2, dW2, db2, dA2, dZ1, dW1, db1 = None, None, None, None, None, None, None\n",
    "\n",
    "    ######################################################################\n",
    "    # TODO: Implement the backward propagation           #\n",
    "    ######################################################################\n",
    "    # Replace pass with your code\n",
    "    dZ2 = A2 - Y  # 交叉熵损失函数对输出层输入的导数\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = dA1 * (Z1 > 0)  # ReLU 激活函数的导数\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "    ######################################################################\n",
    "    #         END OF YOUR CODE               #\n",
    "    ######################################################################\n",
    "\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1o_--PwYxvF"
   },
   "source": [
    "# 1.2.4 Update Parameters\n",
    "\n",
    "Now, we have finished the backward propagation and get all the computed gradients. We can use the computed gradients to update all the parameters using gradient descent methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XcSW4DwsOrSa"
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2  # number of layers\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSDr4s8MaYMa"
   },
   "source": [
    "# 1.2.5 Training our NN\n",
    "\n",
    "In the previous section, we successfully implement the `forward` and `backward` part of NN. Now we can combine all the elements together and train our neural network.\n",
    "\n",
    "In real training process, we don't feed all the data into the neural network at one epoch. Instead, we divide the dataset into small batches and update the parameters using just a small batch of data each time. The introduction of small bacth can reduce the memory load and increase the randomness, which can facilitate the training process and increase the generalization capability of our model. Below we provide two functions that can devide the training set and testing set into small batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "p2GOpSnI00-k"
   },
   "outputs": [],
   "source": [
    "# 将训练集和测试集分成小批量。\n",
    "# 以减少内存负载并增加随机性，这可以促进训练过程并增加我们模型的泛化能力\n",
    "def create_train_batches(X, Y, batch_size):\n",
    "    '''\n",
    "    Inputs:\n",
    "    - X: training data, dimension [784, 60000]\n",
    "    - Y: ground truth label. dimension [10, 60000]\n",
    "    - batch_size: the size of each batch\n",
    "\n",
    "    Outputs:\n",
    "    - mini_batches: a list containing all the data divided into different batches\n",
    "    '''\n",
    "    m = X.shape[1]  # number of data; (60000)\n",
    "    mini_batches = []\n",
    "\n",
    "    # shuffle all the data\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "\n",
    "    # divide the data into batch\n",
    "    num_complete_minibatches = m // batch_size\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k*batch_size : (k+1)*batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*batch_size : (k+1)*batch_size]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "\n",
    "    # deal with the last batch (may not equal to the batch_size)\n",
    "    if m % batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches*batch_size :]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*batch_size :]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "def create_test_batches(X, batch_size):\n",
    "    '''\n",
    "    Inputs:\n",
    "    - X: testing data, dimension [784, 10000]\n",
    "    - batch_size: the size of each batch\n",
    "\n",
    "    Outputs:\n",
    "    - mini_batches: a list containing all the data divided into different batches\n",
    "    '''\n",
    "    mini_batches = []\n",
    "    m = X.shape[1]  # number of data, (10000)\n",
    "    n_batches = m // batch_size\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        X_mini = X[:, i*batch_size:(i+1)*batch_size]\n",
    "        mini_batches.append(X_mini)\n",
    "\n",
    "    if m % batch_size != 0:\n",
    "        X_mini = X[:, n_batches*batch_size:]\n",
    "        mini_batches.append(X_mini)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oK5ofsELdf0P"
   },
   "source": [
    "Before training our NN, we need to define a `model_predict` function to predict the labels of the data samples, which can be further used to compute the prediction accuracy.\n",
    "\n",
    "**Hint 1**: Since our data are seperated into small batches, you need to predict the labels of data in each batch and then concatenate them together to the final predictions\n",
    "\n",
    "**Hint 2**: Recall that output result of our NN is a one-hot probability vector for each data sample. You should convert them into a single label\n",
    "\n",
    "**<font color=\"red\">[Task]</font>**: Fill the **<font color=\"green\">[TODO]</font>** part in the below function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XcQfu2Gcck2H"
   },
   "outputs": [],
   "source": [
    "# 定义一个model_predict函数来预测数据样本的标签，该函数可以进一步用于计算预测精度\n",
    "\n",
    "def model_predict(X_test, parameters, batch_size):\n",
    "    mini_batches = create_test_batches(X_test, batch_size) # 创建测试批次\n",
    "    all_predictions = [] # a list containing the prediction result of each small batch 初始化预测列表\n",
    "\n",
    "    for X_mini in mini_batches: # 循环处理每个批次\n",
    "      ######################################################################\n",
    "      # TODO: predict the results for each small batch       #\n",
    "      ######################################################################\n",
    "      # Replace pass with your code\n",
    "      # 预测每个批次中数据的标签\n",
    "      A2, _ = forward_propagation(X_mini, parameters)\n",
    "      predictions = np.argmax(A2, axis=0)\n",
    "      # 将它们连接在一起以得到最终的预测\n",
    "      all_predictions.append(predictions)\n",
    "\n",
    "\n",
    "      ######################################################################\n",
    "      #         END OF YOUR CODE               #\n",
    "      ######################################################################\n",
    "\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vh8WpO-Ug84O"
   },
   "source": [
    "Finally, we can train our NN. You should following the below training steps to train your NN for each small batch in each iteration :\n",
    "\n",
    "\n",
    "\n",
    "*   Do the **Forward Propagation** to get the forward results\n",
    "*   Compute the **Loss** based on the forward results\n",
    "*   Do the **Backward Propagation** to compute all the gradients\n",
    "*   Update all the parameters\n",
    "\n",
    "After iteration all the small batches, you should calculate the accuracy for both training data and testing data in this iteration.\n",
    "\n",
    "**<font color=\"red\">[Task]</font>**: Fill the **<font color=\"green\">[TODO]</font>** part in the below function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WJG44uxE0ugA"
   },
   "outputs": [],
   "source": [
    "def train(X_train, Y_train, X_test, Y_test, layers_dims, learning_rate, num_iterations, batch_size):\n",
    "    # initialize the parameters\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # training iterations\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # create the small batches\n",
    "        mini_batches = create_train_batches(X_train, Y_train, batch_size)\n",
    "\n",
    "        for mini_batch in mini_batches:\n",
    "            (mini_batch_X, mini_batch_Y) = mini_batch\n",
    "\n",
    "            ######################################################################\n",
    "            # TODO: finish the training process              #\n",
    "            ######################################################################\n",
    "            # Replace pass with your code\n",
    "            # 进行前向传播以获得前向结果\n",
    "            A2, cache = forward_propagation(mini_batch_X, parameters)\n",
    "            # 根据前向结果计算损失\n",
    "            cost = compute_loss(A2, mini_batch_Y)\n",
    "            # 进行反向传播来计算所有梯度\n",
    "            grads = backward_propagation(parameters, cache, mini_batch_X, mini_batch_Y)\n",
    "            # 更新所有参数\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "            ######################################################################\n",
    "            #         END OF YOUR CODE               #\n",
    "            ######################################################################\n",
    "\n",
    "        # acc of training\n",
    "        all_predictions = model_predict(X_train, parameters, batch_size)\n",
    "        train_labels = np.argmax(Y_train, axis=0)\n",
    "        train_acc = np.mean(all_predictions == train_labels)\n",
    "\n",
    "        # predict\n",
    "        all_predictions = model_predict(X_test, parameters, batch_size)\n",
    "        test_labels = np.argmax(Y_test, axis=0)\n",
    "        test_acc = np.mean(all_predictions == test_labels)\n",
    "\n",
    "        # print cost and acc\n",
    "        print (\"Cost after iteration %i: %f | Training Accuracy: %f | Test Accuracy: %f\" %(i+1, cost, train_acc, test_acc))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-FNFPDu1eCk4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 1: 1.599456 | Training Accuracy: 0.571717 | Test Accuracy: 0.565100\n",
      "Cost after iteration 1: 1.599456 | Training Accuracy: 0.571717 | Test Accuracy: 0.565100\n",
      "Cost after iteration 2: 1.378225 | Training Accuracy: 0.642100 | Test Accuracy: 0.633100\n",
      "Cost after iteration 2: 1.378225 | Training Accuracy: 0.642100 | Test Accuracy: 0.633100\n",
      "Cost after iteration 3: 0.996275 | Training Accuracy: 0.671183 | Test Accuracy: 0.668400\n",
      "Cost after iteration 3: 0.996275 | Training Accuracy: 0.671183 | Test Accuracy: 0.668400\n",
      "Cost after iteration 4: 1.624986 | Training Accuracy: 0.693500 | Test Accuracy: 0.693300\n",
      "Cost after iteration 4: 1.624986 | Training Accuracy: 0.693500 | Test Accuracy: 0.693300\n",
      "Cost after iteration 5: 1.144761 | Training Accuracy: 0.715783 | Test Accuracy: 0.715700\n",
      "Cost after iteration 5: 1.144761 | Training Accuracy: 0.715783 | Test Accuracy: 0.715700\n",
      "Cost after iteration 6: 0.864555 | Training Accuracy: 0.733583 | Test Accuracy: 0.732100\n",
      "Cost after iteration 6: 0.864555 | Training Accuracy: 0.733583 | Test Accuracy: 0.732100\n",
      "Cost after iteration 7: 1.121192 | Training Accuracy: 0.750850 | Test Accuracy: 0.747100\n",
      "Cost after iteration 7: 1.121192 | Training Accuracy: 0.750850 | Test Accuracy: 0.747100\n",
      "Cost after iteration 8: 0.794479 | Training Accuracy: 0.762467 | Test Accuracy: 0.760000\n",
      "Cost after iteration 8: 0.794479 | Training Accuracy: 0.762467 | Test Accuracy: 0.760000\n",
      "Cost after iteration 9: 0.720223 | Training Accuracy: 0.772800 | Test Accuracy: 0.770400\n",
      "Cost after iteration 9: 0.720223 | Training Accuracy: 0.772800 | Test Accuracy: 0.770400\n",
      "Cost after iteration 10: 0.609890 | Training Accuracy: 0.783000 | Test Accuracy: 0.781300\n",
      "Cost after iteration 10: 0.609890 | Training Accuracy: 0.783000 | Test Accuracy: 0.781300\n",
      "Cost after iteration 11: 1.434884 | Training Accuracy: 0.790133 | Test Accuracy: 0.788500\n",
      "Cost after iteration 11: 1.434884 | Training Accuracy: 0.790133 | Test Accuracy: 0.788500\n",
      "Cost after iteration 12: 0.828716 | Training Accuracy: 0.795817 | Test Accuracy: 0.794600\n",
      "Cost after iteration 12: 0.828716 | Training Accuracy: 0.795817 | Test Accuracy: 0.794600\n",
      "Cost after iteration 13: 0.657127 | Training Accuracy: 0.803583 | Test Accuracy: 0.803800\n",
      "Cost after iteration 13: 0.657127 | Training Accuracy: 0.803583 | Test Accuracy: 0.803800\n",
      "Cost after iteration 14: 0.839490 | Training Accuracy: 0.804700 | Test Accuracy: 0.805300\n",
      "Cost after iteration 14: 0.839490 | Training Accuracy: 0.804700 | Test Accuracy: 0.805300\n",
      "Cost after iteration 15: 0.785847 | Training Accuracy: 0.808300 | Test Accuracy: 0.806800\n",
      "Cost after iteration 15: 0.785847 | Training Accuracy: 0.808300 | Test Accuracy: 0.806800\n",
      "Cost after iteration 16: 0.761184 | Training Accuracy: 0.815333 | Test Accuracy: 0.812900\n",
      "Cost after iteration 16: 0.761184 | Training Accuracy: 0.815333 | Test Accuracy: 0.812900\n",
      "Cost after iteration 17: 1.210212 | Training Accuracy: 0.814950 | Test Accuracy: 0.815500\n",
      "Cost after iteration 17: 1.210212 | Training Accuracy: 0.814950 | Test Accuracy: 0.815500\n",
      "Cost after iteration 18: 1.188207 | Training Accuracy: 0.819567 | Test Accuracy: 0.816900\n",
      "Cost after iteration 18: 1.188207 | Training Accuracy: 0.819567 | Test Accuracy: 0.816900\n",
      "Cost after iteration 19: 1.432166 | Training Accuracy: 0.821517 | Test Accuracy: 0.820500\n",
      "Cost after iteration 19: 1.432166 | Training Accuracy: 0.821517 | Test Accuracy: 0.820500\n",
      "Cost after iteration 20: 0.895900 | Training Accuracy: 0.822583 | Test Accuracy: 0.820700\n",
      "Cost after iteration 20: 0.895900 | Training Accuracy: 0.822583 | Test Accuracy: 0.820700\n"
     ]
    }
   ],
   "source": [
    "# run this script to train you NN\n",
    "\n",
    "x_train = X_train.values.T # reshape it into [784,60000]\n",
    "y_train = Y_train.values.T\n",
    "x_test = X_test.values.T\n",
    "y_test = Y_test.values.T\n",
    "\n",
    "# origin: para = train(x_train , y_train, x_test, y_test, layer_dims = [784,128,10], learning_rate = 0.01, num_iterations = 20, batch_size = 256)\n",
    "para = train(x_train , y_train, x_test, y_test, layers_dims = [784,128,10], learning_rate = 0.01, num_iterations = 20, batch_size = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hz11VjdxS-As"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8l-Hp7Srks2-"
   },
   "source": [
    "In this exercise, we are going to implement a convoutional neural network from scratch using numpy. The network structure is shown as below:\n",
    "\n",
    "*   `conv_layer`: 32 2*2 convolutional kernels; stride = 1;\n",
    "*   `max_pooling`: 2*2 pooling window; stride = 2;\n",
    "*   `fully-connected layers`: 3 two connected layers, each with n_flatten, 64, 10 nodes. `n_flatten` represents the dimension of the flattened pooling result\n",
    "\n",
    "You will be asked to implement the `forward` part, including `conv_forward`, `pool_forward` and `linear_forward`. The backward part and training process are given. If needed, you can also modify those given functions to align with your implementation.\n",
    "\n",
    "**<font color=\"red\">[Task]</font>**: Fill the **<font color=\"green\">[TODO]</font>** part in the below function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcF_uw9goL7Y"
   },
   "source": [
    "# 2.1 Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HxwqnmY2S9s5"
   },
   "outputs": [],
   "source": [
    "def conv_forward(A_prev, W, b, stride):\n",
    "    \"\"\"\n",
    "    Implement the forward of conv layers (with bias)\n",
    "\n",
    "    Inputs:\n",
    "    - A_prev: The activation output from the previous layer, with dimension of (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    - (1) m is the batch size (2) n_H_prev is the height of previous output\n",
    "    - (3) n_W_prev is the width of the previous output (4) n_C_prev is the number of channels from the previous output\n",
    "\n",
    "    - W: The weight of the conv kernel，with dimension of: (f, f, n_C_prev, n_C)\n",
    "    - (1) f is the size of the kernel (2) n_C is the number of the conv kernels\n",
    "\n",
    "    - b: The bias term with dimension of (1, 1, 1, n_C)\n",
    "    - stride: A integer represents how far the conv kernel moves each time\n",
    "\n",
    "\n",
    "    Outputs:\n",
    "    - Z: The conv result, with dimension of (m, n_H, n_W, n_C)\n",
    "    - A_prev, W, b, stride\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C) = W.shape\n",
    "\n",
    "    #####################################################################################\n",
    "    # TODO: Compute the height and weight of the output after convolution  #\n",
    "    # Replace None with your formula                       #\n",
    "    #####################################################################################\n",
    "    # 计算卷积操作后输出特征图的高度和宽度\n",
    "    # 高度/宽度 方向上扫过的最大范围 除以 步数， 加1是因为从输入的起始位置（index 0）开始计数，需要包括这一位置在内的卷积操作\n",
    "    n_H = (n_H_prev - f)//stride + 1\n",
    "    n_W = (n_W_prev - f)//stride + 1\n",
    "\n",
    "\n",
    "    # initialize the output\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "\n",
    "    #####################################################################################\n",
    "    # TODO: Implement the convolutional process                  #\n",
    "    # Hint: You can use a for loop to iterate all samples, H, W, C    #\n",
    "    #####################################################################################\n",
    "\n",
    "    for i in range(m): \n",
    "        for h in range(n_H): \n",
    "            for w in range(n_W): \n",
    "                for c in range(n_C): \n",
    "                    # 起始和结束位置\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    vert_start = h * stride \n",
    "                    vert_end = vert_start + f\n",
    "                    \n",
    "                    # 切片来提取子矩阵\n",
    "                    # 样本num 垂直方向切片段 水平方向切片段\n",
    "                    a_slice_prev = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    #a_slice_prev 中包含的是输入特征图的局部数据，\n",
    "                    # W[:, :, :, c] 是对应于输出特征图第 c 通道的卷积核。这个卷积核将遍历输入特征图，以生成输出特征图的一个元素\n",
    "                    #对结果矩阵的所有元素求和。这一步实质上是完成了卷积操作，即加权和。\n",
    "                        # 通过对所有元素求和，将这个卷积核与局部输入的所有对应元素进行加权，得到一个单一的数值，\n",
    "                        # 这个数值代表了在输出特征图的位置 (h, w) 上，通道 c 的激活值。\n",
    "                    # 最后将计算得到的加权和加上一个偏置值， 更好拟合数据\n",
    "                    Z[i, h, w, c] = np.sum(a_slice_prev * W[:, :, c]) + b[0, 0, 0, c]\n",
    "\n",
    "    #####################################################################################\n",
    "    #             End of Your Code                   #\n",
    "    #####################################################################################\n",
    "\n",
    "\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "    return Z, A_prev, W, b, stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9STy2sD1TECp"
   },
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, f, stride):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - A_prev: The activation output from the previous layer, with dimension of (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    - (1) m is the batch size (2) n_H_prev is the height of previous output\n",
    "    - (3) n_W_prev is the width of the previous output (4) n_C_prev is the number of channels from the previous output\n",
    "\n",
    "    - f: Integer, the height and width of the pooling windows\n",
    "    - stride: Integer, indicate how far the pooling move each time\n",
    "\n",
    "    Outputs:\n",
    "    - A: The output of the pooling layers, with dimension of (m, n_H, n_W, n_C)\n",
    "    - A_prev, f, stride, n_H, n_W, n_C\n",
    "\n",
    "    \"\"\"\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    #####################################################################################\n",
    "    # TODO: Compute the height and weight of the output after convolution  #\n",
    "    # Replace None with your formula                       #\n",
    "    #####################################################################################\n",
    "    n_H = (n_H_prev - f)//stride + 1\n",
    "    n_W = (n_W_prev - f)//stride + 1\n",
    "    n_C = n_C_prev\n",
    "\n",
    "    # initialize the output\n",
    "    A = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "    #####################################################################################\n",
    "    # TODO: Implement the Pooling process                     #\n",
    "    # Hint: You can use a for loop to iterate all samples, H, W, C    #\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(n_H):\n",
    "            for k in range(n_W):\n",
    "                for l in range(n_C):\n",
    "                    horiz_start = k * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    vert_start = j * stride\n",
    "                    vert_end = vert_start + f\n",
    "        \n",
    "                    a_slice_prev = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, l]                  \n",
    "                    A[i , j , k , l] = np.max(a_slice_prev)\n",
    "    \n",
    "    \n",
    "    #####################################################################################\n",
    "    #####################################################################################\n",
    "    #             End of Your Code                   #\n",
    "    #####################################################################################\n",
    "\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "\n",
    "    return A, A_prev, f, stride, n_H, n_W, n_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9iKLb3lqTG6j"
   },
   "outputs": [],
   "source": [
    "def fully_connected_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - A_prev: activation output from the last layer，with a dimension of (batch_size, # of nodes in last layer)\n",
    "    - W: Weigh Matrix, with a dimension of (# of nodes in last layer, # of nodes in this layer)\n",
    "    - b: Bias Term, with a dimension of (# of nodes in this layer, 1)\n",
    "\n",
    "    Outputs:\n",
    "    - Z: The output result without activation\n",
    "    \"\"\"\n",
    "\n",
    "    Z = None\n",
    "    #####################################################################################\n",
    "    # TODO: Implement the fc forward process                   #\n",
    "    #####################################################################################\n",
    "   \n",
    "    \n",
    "    Z = np.dot(A_prev, W) + b\n",
    "\n",
    "    #####################################################################################\n",
    "    #             End of Your Code                   #\n",
    "    #####################################################################################\n",
    "\n",
    "    return Z, W, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "-XqkzIXnTKK5"
   },
   "outputs": [],
   "source": [
    "def cnn_forward(X, parameters):\n",
    "\n",
    "    cache = []\n",
    "\n",
    "    # First Conv\n",
    "    Z1, A0, W1, b1, s1 = conv_forward(X, parameters['W1'], parameters['b1'], stride=1)\n",
    "    A1 = relu(Z1)\n",
    "\n",
    "    # Max Pooling\n",
    "    P1, A1, f, s2, H2, W2, C2 = pool_forward(A1, f=2, stride=2)\n",
    "\n",
    "    # Flatten\n",
    "    P1_flattened = P1.reshape(P1.shape[0], -1)\n",
    "\n",
    "    # FC Layers 1\n",
    "    Z2, W3, b3 = fully_connected_forward(P1_flattened, parameters['W2'], parameters['b2'])\n",
    "    A2 = relu(Z2)\n",
    "\n",
    "    # FC Layers 2\n",
    "    Z3, W4, b4 = fully_connected_forward(A2, parameters['W3'], parameters['b3'])\n",
    "    A3 = softmax(Z3)\n",
    "\n",
    "    # conv cache --> pooling cache --> fc 1 cache --> fc 2 cache\n",
    "    cache.append((Z1, A0, W1, b1, s1))\n",
    "    cache.append((P1, A1, f, s2, H2, W2, C2))\n",
    "    cache.append((Z2, P1_flattened, W3, b3))\n",
    "    cache.append((Z3, A2, W4, b4))\n",
    "\n",
    "    return A3, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "moG49F1cuCW1"
   },
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "v7wTg6hWVNnz"
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    Z, A_prev, W, b = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "def conv_backward(dZ, cache):\n",
    "    (Z, A_prev, W, b, stride) = cache\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    (f, f, n_C) = W.shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "\n",
    "    # initialize the gradients\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    dW = np.zeros_like(W)\n",
    "    db = np.zeros_like(b)\n",
    "\n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    # localize the current slice\n",
    "                    x_start = h * stride\n",
    "                    x_end = x_start + f\n",
    "                    y_start = w * stride\n",
    "                    y_end = y_start + f\n",
    "                    a_slice = a_prev[x_start:x_end, y_start:y_end, :]\n",
    "\n",
    "                    # compute the gradients\n",
    "                    dA_prev[i, x_start:x_end, y_start:y_end, :] += (W[:, :, c] * dZ[i, h, w, c]).reshape(2,2,1)\n",
    "                    dW[:, :, c] += (a_slice * dZ[i, h, w, c]).reshape(2,2)\n",
    "                    db[:, :, :, c] += dZ[i, h, w, c]\n",
    "\n",
    "    # compute the bias gradients\n",
    "    db = np.sum(dZ, axis=(0, 1, 2), keepdims=True) / m\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "\n",
    "def create_mask_from_window(x):\n",
    "    mask = x == np.max(x)\n",
    "    return mask\n",
    "\n",
    "def pool_backward(dA, cache):\n",
    "    (P, A_prev, f, stride, n_H, n_W, n_C) = cache\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    bs = dA.shape[0]\n",
    "    dA = dA.reshape(bs ,n_H ,n_W ,n_C) # unflatten\n",
    "\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "\n",
    "    for i in range(m):\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):\n",
    "            for w in range(n_W):\n",
    "                for c in range(n_C):\n",
    "                    x_start = h * stride\n",
    "                    x_end = x_start + f\n",
    "                    y_start = w * stride\n",
    "                    y_end = y_start + f\n",
    "                    a_prev_slice = a_prev[x_start:x_end, y_start:y_end, c]\n",
    "                    mask = create_mask_from_window(a_prev_slice)\n",
    "                    dA_prev[i, x_start:x_end, y_start:y_end, c] += mask * dA[i, h, w, c]\n",
    "    return dA_prev\n",
    "\n",
    "def distribute_value(dz, shape):\n",
    "    (n_H, n_W) = shape\n",
    "    average = dz / (n_H * n_W)\n",
    "    a = np.ones(shape) * average\n",
    "    return a\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    Z, A_prev, W, b= cache\n",
    "    m = A_prev.shape[0]\n",
    "    dW = 1./m * np.dot(A_prev.T, dZ)\n",
    "    db = 1./m * np.sum(dZ, axis=0, keepdims=True)\n",
    "    dA_prev = np.dot(dZ,W.T)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def cnn_backward(AL, Y, caches):\n",
    "\n",
    "    gradients = {}\n",
    "\n",
    "    # compute the gradient of the output layer dAL which uses the softmax activation\n",
    "    dAL = AL - Y\n",
    "\n",
    "\n",
    "    # backward of the second fc\n",
    "    current_cache = caches[-1]\n",
    "    gradients[\"dA2\"], gradients[\"dW3\"], gradients[\"db3\"] = linear_backward(dAL, current_cache)\n",
    "\n",
    "    # backward of the relu function + the first lc\n",
    "    current_cache = caches[-2]\n",
    "    dZ = relu_backward(gradients[\"dA2\"], current_cache)\n",
    "    gradients[\"dA1\"], gradients[\"dW2\"], gradients[\"db2\"] = linear_backward(dZ, current_cache)\n",
    "    # print(gradients[\"dW2\"].shape)\n",
    "\n",
    "    # backward of max pooling --> relu --> backward of conv\n",
    "    current_cache = caches[-3]\n",
    "    dA0 = pool_backward(gradients[\"dA1\"], current_cache)\n",
    "\n",
    "    Z1, A0, W1, b1, s1 = caches[0]\n",
    "    current_cache = (Z1, A0, W1, b1)\n",
    "    dZ = relu_backward(dA0, current_cache)\n",
    "\n",
    "    current_cache = caches[0]\n",
    "    gradients[\"dA0\"], gradients[\"dW1\"], gradients[\"db1\"] = conv_backward(dZ, current_cache)\n",
    "\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLr7Ho5uwAPO"
   },
   "source": [
    "# Update Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Pmak-ncXafRy"
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2  # number of layers\n",
    "\n",
    "    for l in range(1, L+1):\n",
    "        parameters[\"W\" + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HohRhwXwO2P"
   },
   "source": [
    "# Creating Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "14S9h_8e6GGF"
   },
   "outputs": [],
   "source": [
    "def create_minibatches(X, Y, minibatch_size):\n",
    "\n",
    "    m = X.shape[0]\n",
    "    minibatches = []\n",
    "\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :, :, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "\n",
    "    num_complete_minibatches = m // minibatch_size\n",
    "\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        minibatch_X = shuffled_X[k * minibatch_size:(k + 1) * minibatch_size, :, :, :]\n",
    "        minibatch_Y = shuffled_Y[k * minibatch_size:(k + 1) * minibatch_size, :]\n",
    "        minibatch = (minibatch_X, minibatch_Y)\n",
    "        minibatches.append(minibatch)\n",
    "\n",
    "    if m % minibatch_size != 0:\n",
    "        minibatch_X = shuffled_X[num_complete_minibatches * minibatch_size:, :, :, :]\n",
    "        minibatch_Y = shuffled_Y[num_complete_minibatches * minibatch_size:, :]\n",
    "        minibatch = (minibatch_X, minibatch_Y)\n",
    "        minibatches.append(minibatch)\n",
    "\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGvPibHsxQaK"
   },
   "source": [
    "# Training Your CNN\n",
    "\n",
    "**<font color=\"red\">[Task]</font>**: Fill the **<font color=\"green\">[TODO]</font>** part to complete the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "2QlAQtdTWEqJ"
   },
   "outputs": [],
   "source": [
    "# The first step is initializing the parameters\n",
    "\n",
    "parameters = {}\n",
    "\n",
    "# Initilalize the parameters of conv kernels\n",
    "parameters['W' + str(1)] = np.random.randn(2, 2, 16) * np.sqrt(2/68)\n",
    "parameters['b' + str(1)] = np.zeros((1, 1, 1, 16))\n",
    "\n",
    "\n",
    "#############################################################################################################\n",
    "# TODO: Based on the network structure, compute the dimension of the flatten pooling result  #\n",
    "# Replaced None with your computed result                               #\n",
    "##############################################################################################################\n",
    "# 13*13*16\n",
    "n_L_prev = 2704\n",
    "\n",
    "\n",
    "# Initialize the paramters of the FC Layers\n",
    "parameters['W' + str(2)] = np.random.randn(n_L_prev, 64) * np.sqrt(2 / (n_L_prev + 64))\n",
    "parameters['b' + str(2)] = np.zeros((1,64))\n",
    "parameters['W' + str(3)] = np.random.randn(64, 10) * np.sqrt(2 / (64+10))\n",
    "parameters['b' + str(3)] = np.zeros((1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "k5eQrgdSXQFu"
   },
   "outputs": [],
   "source": [
    "def predict_accuracy(X, Y, parameters):\n",
    "    probas, caches = cnn_forward(X, parameters)\n",
    "    predicted_labels = np.argmax(probas, axis=1)\n",
    "    true_labels = np.argmax(Y, axis=1)\n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "    return accuracy\n",
    "\n",
    "def train_cnn(X_train, Y_train, X_test, Y_test, parameters, learning_rate=0.001, num_epochs=100, batch_size=64):\n",
    "\n",
    "    costs = []  # list that stores cost\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "        mini_batches = create_minibatches(X_train, Y_train, batch_size)\n",
    "        cost_total = 0\n",
    "\n",
    "        for minibatch in mini_batches:\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            ######################################################################\n",
    "            # TODO: finish the training process              #\n",
    "            ######################################################################\n",
    "            # Replace pass with your code\n",
    "            probas, caches = cnn_forward(minibatch_X, parameters)\n",
    "            \n",
    "            # 计算损失函数\n",
    "            cost = compute_loss(probas, minibatch_Y)\n",
    "            cost_total += cost\n",
    "            \n",
    "            # 反向传播\n",
    "            grads = cnn_backward(probas, minibatch_Y, caches)\n",
    "            \n",
    "            # 更新参数\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "            \n",
    "            ######################################################################\n",
    "            #         END OF YOUR CODE               #\n",
    "            ######################################################################\n",
    "\n",
    "        cost_avg = cost_total / len(mini_batches)\n",
    "        costs.append(cost_avg)\n",
    "\n",
    "        # Print the Accuracy\n",
    "\n",
    "        test_accuracy = predict_accuracy(X_test, Y_test, parameters)\n",
    "        print(f\"Epoch {i+1}/{num_epochs}, Cost: {cost_avg}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "tOZTMigf2ipH"
   },
   "outputs": [],
   "source": [
    "# get all the training and testing data\n",
    "x_train, x_test = reshape_data(X_train.values, X_test.values)\n",
    "y_train, y_test = Y_train.values, Y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFqqoGCbyZrW"
   },
   "source": [
    "Run this cell to train you CNN. Since we use Numpy to implement CNN from scratch, the training speed may not be fast. Therefore, when testing your data, you can use a very small dataset to verify all your codes run well. For example, you can use 20 training samples and 1 test samples. After that, you can use a larger subset of the dataset to present your results.\n",
    "\n",
    "Since this CNN is trained on a subset of the whole dataset, you get the marks if the result is reasonable.\n",
    "\n",
    "**Reference**: Using 1000 training samples and 100 testing samples for 5 epochs will take like 40mins in TA's vanilla implementation; Using 100 training samples and 10 testing samples for 5 epochs will take 3-5 mins. It's normal that the model becomes overfitting if you only use small subset of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoPRyoCtZ_Up"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Cost: 47.07439462967633, Test Accuracy: 0.03\n",
      "Epoch 1/10, Cost: 47.07439462967633, Test Accuracy: 0.03\n",
      "Epoch 2/10, Cost: 46.96865093284815, Test Accuracy: 0.04\n",
      "Epoch 2/10, Cost: 46.96865093284815, Test Accuracy: 0.04\n",
      "Epoch 3/10, Cost: 46.93502069852417, Test Accuracy: 0.04\n",
      "Epoch 3/10, Cost: 46.93502069852417, Test Accuracy: 0.04\n",
      "Epoch 4/10, Cost: 46.90217878674325, Test Accuracy: 0.05\n",
      "Epoch 4/10, Cost: 46.90217878674325, Test Accuracy: 0.05\n",
      "Epoch 5/10, Cost: 46.91752886401083, Test Accuracy: 0.05\n",
      "Epoch 5/10, Cost: 46.91752886401083, Test Accuracy: 0.05\n",
      "Epoch 6/10, Cost: 46.97193304980562, Test Accuracy: 0.05\n",
      "Epoch 6/10, Cost: 46.97193304980562, Test Accuracy: 0.05\n",
      "Epoch 7/10, Cost: 46.940672953246434, Test Accuracy: 0.06\n",
      "Epoch 7/10, Cost: 46.940672953246434, Test Accuracy: 0.06\n"
     ]
    }
   ],
   "source": [
    "# Replace all the None to the subset of the dataset\n",
    "x_train_subset = x_train[:200]\n",
    "y_train_subset = y_train[:200]\n",
    "x_test_subset = x_test[:100]     \n",
    "y_test_subset = y_test[:100]\n",
    "para, cost = train_cnn(x_train_subset, y_train_subset, x_test_subset, y_test_subset, parameters, learning_rate=0.002 , num_epochs=10, batch_size=128)                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOjy408ew0WEW+5EMQKv2NY",
   "mount_file_id": "1AdGyiVYosU2uMBQ9ahx5B7ZOmH8zEQg_",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
